11

#### 线性关系

$$
\vec{w}=[w_0,w_1,w_2,\cdots,w_n] \\

\vec{x}=[x_0,x_1,x_2,\cdots,x_n] \\

f(x)= \sum_{i \in N}w_ix_i+b =w_0x_0+w_1+x_1+\cdots+w_nx_n+b=w^Tx+b
$$



$x \in R^1,f(x)$  is a liner

$x \in R^2,f(x)$  is a plane.

任务：预测波士顿房价 

步骤1：拿到数据集，观察数据

```python
import random
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.datasets import load_boston
mydataset = load_boston()
print(mydataset)

'''
{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,
......
'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,
......
'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',
       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'), 
'''


```



步骤2：提取数据，并转换成pandas格式

```python
data = mydataset['data']
target = mydataset['target']
columns = mydataset['feature_names']

mydataframe=pd.DataFrame(data)
mydataframe.columns=columns
mydataframe['price']=target

print(mydataframe)
print(mydataframe.shape)
```

<img src="C3.assets/image-20210604133151455.png" alt="image-20210604133151455" style="zoom:80%;" />



步骤3：查看、观察数据相关性

```python

print(mydataframe.corr())
sns.heatmap(mydataframe.corr())
plt.show()
# RM：小区平均的卧室个数
# LSTAT: 低收入人群在周围的比例
# correlation => 如果一个值的增大，会引起另外一个值一定增大，而且是定比例增大 相关系数就越接近于1
# correlation => 0 就是两者之间没有任何关系
# correlation => -1 一个值增大 另外一个值一定减小 而且减小是成相等比例的
```

<img src="C3.assets/image-20210604133536160.png" alt="image-20210604133536160" style="zoom:80%;" />

<img src="C3.assets/image-20210604135755236.png" alt="image-20210604135755236" style="zoom:67%;" />

结论：对房价影响最大的是RM（正相关）和LSTAT（负相关）

步骤4：提取关键数据RM和LSTAT

```python
rm=mydataframe['RM']
lstat =mydataframe['LSTAT']
print(rm)
'''
0      6.575
1      6.421
2      7.185
       ...
504    6.794
505    6.030
Name: RM, Length: 506, dtype: float64
'''
```

步骤5：抽象和简化模型（即房价和RM是线性关系）

```python
def model():
    return np.dot(x, w.T) + b  #矩阵转置和矩阵点积

#x 向量即矩阵x，(rm ,lstat)
#w 向量即矩阵w, (w1,w2) 权重
#b 即偏置b,
```



步骤6：定义损失函数（代价函数）

[复习线性回归相关概念](#线性回归具体思路和流程)

```python
def loss(yhat, y):
    return np.mean(0.5* (yhat - y) ** 2)

#预测值: yhat
#训练数据的正解（price）：y
#numpy的妙用，直接用向量计算（矩阵计算）
```



步骤7：定义损失函数关于$w$ 的偏导函数 ，同理，定义损失函b数关于$b$ 的偏导函数 

```python
def partial_w(x, y, yhat):
    return np.array([np.mean((yhat - y) * x[0]), np.mean((yhat - y) * x[1])])
#向量x[rm,lstat]  x[0]即rm ,x[1]即lstat
#返回的是numpy数组

def partial_b(x, y, yhat):
    return np.mean((yhat - y))

```



步骤8： 定义训练函数，初始话$w$和$b$  ，权重$w:(w_1,w_2)$

```python
def train(model_to_be_train, target, loss, pw, pb):

    w = np.random.random_sample((1, 2)) # w normal
    b = np.random.random() # 0 深度学习的时候会和大家详细解释
    learning_rate = 1e-5  #学习率
    epoch = 200  #训练次数
    losses = []

    for i in range(epoch):
        batch_loss = []
        for batch in range(len(rm)):
            # batch training
            index = random.choice(range(len(rm)))  #随机的取训练数据，
            rm_x, lstat_x = rm[index], lstat[index]
            x = np.array([rm_x, lstat_x])  #组装成np数组
            y = target[index] #对应的正解

            yhat = model_to_be_train(x, w, b)  #计算预测值
            loss_v = loss(yhat, y) #计算当前的损失函数值

            batch_loss.append(loss_v) #追加到np数组中

            w, b = optimize(w, b, x, y, yhat, pw, pb, learning_rate)

            if batch % 100 == 0:
                print('Epoch: {} Batch: {}, loss: {}'.format(i, batch, loss_v))
        losses.append(np.mean(batch_loss))

    return model_to_be_train, w, b, losses
```





### 多变量函数的近似公式（梯度下降的基础）

梯度下降法是确定神经网络的一种代表性的方法。在应用梯度下降法时，需要用到多变量函数的近似公式。

#### 单变量函数的近似公式

已知导数定义：
$$
f′(x) =\lim_{\Delta x\rightarrow 0}\frac{f(x+ \Delta x)-f(x)}{\Delta x}{}
$$
将$\Delta x$这个”无限趋于0“或者说”无限小“的量，替换成”微小”的量，得到：
$$
f′(x) \approx \frac{f(x+ \Delta x)-f(x)}{\Delta x}  \\
$$
整理得：
$$
f(x+ \Delta x)\approx  f(x)+ f′(x){\Delta x}
$$
这个就是，**单变量函数的近似公式**，这里$\Delta x$是"微小得数"

举例：$y=f(x)=e^{x}$ ，求其 $x=0$ 附近 得近似公式
$$
(e^x)'=e^x  \\
e^{x+\Delta x} \approx  e^{x}+e^{x}\Delta x\\
x\rightarrow0\\
e^x\approx 1+\Delta x \approx 1+x
$$
得到：$y=e^x$  和其在$x\rightarrow 0$时得近似函数 $y=1+x$

如图：

<img src="C3.assets/image-20210528131611536-1622178972633.png" alt="image-20210528131611536" style="zoom:80%;" />



#### 多变量函数的近似公式

将单变量函数的近似公式扩展到两个变量的函数。如果$x$、$y$ 作微小的变化，那么函数 $z = f(x, y)$ 的值将会怎样变化呢？答案就是**多变量函数近似公式**。*∆x*、*∆y* 为微小的数。
$$
f(x+ \Delta x，y+ \Delta y)\approx  f(x,y)+ \frac{\partial f(x,y)}{\partial x}{\Delta x}+\frac{\partial f(x,y)}{\partial y}{\Delta y}
$$
化简一下：定义$\Delta z =f(x+ \Delta x，y+ \Delta y)-f(x,y) $   得到：
$$
\Delta z \approx \frac{\partial z}{\partial x}\Delta x +\frac{\partial z}{\partial y}\Delta y
$$
通过这样的简化方式，就很容易将近似公式进行推广到哪个变量$x_1,x_2,\cdots ,x_n$。

例如，变量 $z$为四个变量 $w,x,y,b$的函数时，近似公式如下所示
$$
\Delta z \approx \frac{\partial z}{\partial w}\Delta w +\frac{\partial z}{\partial x}\Delta x +\frac{\partial z}{\partial y}\Delta y+\frac{\partial z}{\partial b}\Delta b
$$

#### 近似公式的向量表示

如上，四个变量的函数的近似公式 ，可以表示为如下两个向量的内积：（复习一下向量内积的坐标表示）
$$
梯度向量：
\nabla \alpha =( \frac{\partial z}{\partial w} ,\frac{\partial z}{\partial x},\frac{\partial z}{\partial y},\frac{\partial z}{\partial b})\\

位移向量：\Delta \beta =(\Delta w ,\Delta x ,\Delta y ,\Delta b )\\
$$
对于一般的 *n* 变量函数，近似公式也可以像这样表示为内积的形式





### 最优化问题和回归分析

从数学上来说，确定神经网络的参数是一个最优化问题，具体就是对神经网络的参数（即权重和偏置）进行拟合，使得神经网络的输出与实际数据相吻合。

为了理解最优化问题，最浅显的例子就是回归分析。

#### 什么是回归分析

由多个变量组成的数据中，着眼于其中一个特定的变量，用其余的变量来解释这个特定的变量，这样的方法称为回归分析。回归分析的种类有

很多。

最简单的**线性回归分析**，以两个变量组成的数据作为考察对象，用一条直线近似地表示右图所示的散点图上的点列，通过该直线的方程来考察两个变量之间的关系。该直线称为**回归直线**

有训练数据如下：

| 身高（x) | 体重（y) |
| -------- | -------- |
| 174.1    | 61.5     |
| $\cdots$ | $\cdots$ |
| 177.8    | 66.1     |

#### 线性回归具体思路和流程

回归方程，x是自变量，y是因变量，模型（model）： $y=wx+b$

训练数据：自变量$x_i$，因变量  $y_i$

预测值：$\hat{y}=wx+b$

预测值和实际测试数据的误差：${y_i}-\hat{y_i}=y_i-(wx_i+b)$

平方误差：$C_i=\frac{1}{2}({y_i}-(wx_i+b))^2$

平方误差和（损失函数，代价函数）：$loss(x,y)=\sum_{i=1}^n\frac{1}{2}({y_i}-(wx_i+b))^2 $

实际工作中，考虑不同训练数据集合中数据规模不同（即$n$不同），为了对比不同训练结果，损失函数定义为：
$$
loss(x,y)=\frac{1}{n}\sum_{i=1}^n\frac{1}{2}({y_i}-(wx_i+b))^2
$$
在这个简单的线性回归中，我们的目标是：找到合适的$w,b$，尽可能让损失函数$loass(x,y)$取最小值0，即：
$$
\frac{\partial loss(x,y)}{\partial w}=0  \qquad \frac{\partial loss(x,y)}{\partial b}=0
$$
根据复合函数链式偏导法则，得到：
$$
\frac{\partial loss(x,y)}{\partial w}= \sum_{i=1}^n  x_i(y_i-(wx_i+b))=0\\
\qquad \frac{\partial loss(x,y)}{\partial b}=\sum_{i=1}^n  -(y_i-(wx_i+b))=0
$$
联立求解$w,b$

**总结：**

综上，机器学习过程中

步骤1：确定了模型（线性回归直线模型中只有两个参数$w,b$），即，根据身高和体重的训练数据去找到预测体重的回归直线方程

步骤2：通过大量训练数据求累加和，使其等于0（或趋近于0），联立求解，得到模型中的参数$w,b$ ，完成机器学习

注意1：当模型中参数增多，要提供更多的训练数据，数据规模

注意2：训练数据的采集和使用前，要预先进行数据标准化和数据清洗

注意3：损失函数有很多种形式，本例中损失函数是平方误差总和（最小二乘法），最小值为0，本例中代价函数关于$w$和$b$两个参数求偏导，如果参数量巨大会怎样？

